---
title: Supporting information
author:
  - >
    Alexey N. Shiklomanov,
    Elizabeth M. Cowdery,
    Michael Bahn,
    Chaeho Byun,
    Steven Jansen,
    Koen Kramer,
    Vanessa Minden,
    Ãœlo Niinemets,
    Yusuke Onoda,
    Nadejda A. Soudzilovskaia,
    Michael C. Dietze
output: pdf_document
header-includes:
  - \usepackage{booktabs}
  - \usepackage{longtable}
  - \usepackage{array}
  - \usepackage{multirow}
  - \usepackage{wrapfig}
  - \usepackage{float}
  - \usepackage{colortbl}
  - \usepackage{pdflscape}
  - \usepackage{tabu}
  - \usepackage{threeparttable}
  - \usepackage{threeparttablex}
  - \usepackage[normalem]{ulem}
  - \usepackage{makecell}
  - \renewcommand{\thetable}{S\arabic{table}}
  - \renewcommand{\thefigure}{S\arabic{figure}}
---

```{r libraries, include = FALSE}
library(knitr)
library(kableExtra)
library(drake)
library(ggplot2)
library(GGally)
stopifnot(
  requireNamespace("shiklomanov2017np", quietly = TRUE),
  requireNamespace("dplyr", quietly = TRUE),
  requireNamespace("tibble", quietly = TRUE)
)

correlation_scatter_cap <- paste(
  "Relationship between squared pairwise correlation and sample size of pairwise observations for each PFT.",
  "Grey shading is the 95% confidence interval around a local regression (LOESS).",
  "Upper triangle is area-normalized values, and lower triangle is mass-normalized values.",
  "Note that the x-axis uses a $\\log_{10}$ scale."
)

correlation_fig_cap <- paste(
  "Fraction of statistically significant ($p < 0.05$) pairwise trait correlation estimates",
  "as a function of sample size for each plant functional type."
)

correlation_range_cap <- paste(
  "Relationship between pairwise correlation strength and data range for each PFT.",
  "In each pair of sub-panels, the top panel is relationship between pairwise correlation",
  "and the range of the variable on the x axis",
  "while the bottom panel is the relationship between pairwise correlation",
  "and the y variable",
  "(For instance, in the two-panel figure in row 1 column 2,",
  "the top panel is the SLA-leaf lifespan correlation as a function of the range of SLA",
  "and the bottom panel is the same correlation as a function of the range of leaf lifespan).",
  "Data range is expressed as the difference between the 2.5% and 97.5% quantiles of each normalized trait (i.e. $\\frac{X}{E[X]}$).",
  "Grey shading is the 95% confidence interval around a local regression (LOESS).",
  "Upper triangle is area-normalized values, and lower triangle is mass-normalized values."
)
```

```{r echo = FALSE, results = "asis"}
prior_df <- tibble::tribble(
  ~param, ~mean, ~stdev,
  "Jmax_area", 2, 0.19,
  "Jmax_mass", 0, 0.26,
  "leaf_lifespan", 0.8, 0.4,
  "Narea", 0.25, 0.3,
  "Nmass", 1.25, 0.3,
  "Parea", -1, 0.3,
  "Pmass", 0, 0.3,
  "Rdarea", log10(0.0011), 0.15,
  "Rdmass", log10(0.8), 0.20,
  "SLA", 1, 0.6,
  "Vcmax_mass", log10(0.45), 0.2,
  "Vcmax_area", log10(50), 0.18
) %>%
  dplyr::mutate(variance = stdev ^ 2)

prior_df %>%
  dplyr::mutate(param = factor(param, shiklomanov2017np::both_params)) %>%
  dplyr::arrange(param) %>%
  dplyr::select(Trait = param, "Lognormal Mean" = mean, "Lognormal SD" = stdev) %>%
  knitr::kable(
    format = "latex",
    booktabs = TRUE,
    caption = paste(
      "Parameters for trait-specific prior distributions.",
      "For the multivariate and hierarchical distributions,",
      "the variance-covariance matrix prior was a diagonal matrix containing the",
      "square of the lognormal standard deviation values (i.e. the variance)",
      "shown here and all off-diagnoal terms set to zero." 
    )
  ) %>%
  kableExtra::kable_styling(latex_options = c("scale_down", "hold_position"))
```


```{r mean_value_table, echo = FALSE, results = "asis"}
table_cap_mass <- paste(
  "Mean and 95\\% confidence interval of trait estimates",
  "for mass-normalized traits from the hierarchical model.",
  "Note that some traits for some PFTs have effectively no constraint relative to the mostly uninformative prior (as evidenced by their very large confidence intervals),",
  "so their values should be used with caution."
)
table_cap_area <- gsub("mass", "area", table_cap_mass)

readd(table_dat) %>%
  dplyr::select(-dplyr::matches("area")) %>%
  kable(
    caption = table_cap_mass,
    format = "latex",
    col.names = linebreak(c(
      "PFT",
      "Leaf lifespan\n(months)",
      "SLA\n(kg m$^{-2}$)",
      "$N_\\textrm{mass}$\n(mg g$^{-1}$)",
      "$P_\\textrm{mass}$\n(mg g$^{-1}$)",
      "$R_\\textrm{d, mass}$\n($\\mu$mol g$^{-1}$ s$^{-1}$)",
      "$V_\\textrm{c, max, mass}$\n($\\mu$mol g$^{-1}$ s$^{-1}$)",
      "$J_\\textrm{max, mass}$\n($\\mu$mol g$^{-1}$ s$^{-1}$)"
    )),
    escape = FALSE,
    booktabs = TRUE
  ) %>%
  kableExtra::kable_styling(latex_options = c("scale_down", "hold_position"))

readd(table_dat) %>%
  dplyr::select(-dplyr::matches("mass")) %>%
  kable(
	caption = table_cap_area,
	format = "latex",
    col.names = linebreak(c(
      "PFT",
      "Leaf lifespan\n(months)",
      "SLA\n(kg m$^{-2}$)",
      "$N_\\textrm{area}$\n(g m$^{-2}$)",
      "$P_\\textrm{area}$\n(g m$^{-2}$)",
      "$R_\\textrm{d, area}$\n($\\mu$mol m$^{-2}$ s$^{-1}$)",
      "$V_\\textrm{c, max, area}$\n($\\mu$mol m$^{-2}$ s$^{-1}$)",
      "$J_\\textrm{max, area}$\n($\\mu$mol m$^{-2}$ s$^{-1}$)"
    )),
	escape = FALSE,
	booktabs = TRUE
) %>%
  kableExtra::kable_styling(latex_options = c("scale_down", "hold_position"))
```

```{r sample_size_table, echo = FALSE, results = "asis"}
ssw <- readd(sample_size_wide)
kable(
  ssw %>%
    dplyr::select(pft, leaf_lifespan, SLA,
                  Nmass, Narea, Pmass, Parea, Rdmass, Rdarea,
                  Vcmax_mass, Vcmax_area, Jmax_mass, Jmax_area),
  col.names = c(
    "PFT", "Leaf lifespan", "SLA",
    "$N_\\textrm{mass}$", "$N_\\textrm{area}$",
    "$P_\\textrm{mass}$", "$P_\\textrm{area}$",
    "$R_\\textrm{d, mass}$", "$R_\\textrm{d, area}$",
    "$V_\\textrm{c, max, mass}$", "$V_\\textrm{c, max, area}$",
    "$J_\\textrm{max, mass}$", "$J_\\textrm{max, area}$"
  ),
  format = "latex",
  booktabs = TRUE,
  escape = FALSE,
  caption = "The number of non-missing (outside parentheses) and imputed (inside parentheses) observations of each trait for each plant functional type used in this analysis."
) %>%
  kable_styling(latex_options = c("scale_down", "hold_position")) %>%
  group_rows("Broadleaf", 2, 6) %>%
  group_rows("Needleleaf", 7, 9) %>%
  group_rows("Shrub", 10, 12) %>%
  group_rows("Grass", 13, 15) %>%
  landscape()
```

```{r pairwise_correlation_table, echo = FALSE, results = "asis"}
corr_results <- readd(corr_processed)
kable(
  corr_results %>%
    dplyr::select(pft, yvar, xvar, corr_value, present, missing),
  "latex",
  booktabs = TRUE, longtable = TRUE, escape = TRUE,
  col.names = c("PFT", "Trait 1", "Trait 2", "Correlation (95% CI)", "present", "missing"),
  caption = "Pairwise trait correlation values for each plant functional type. Values with asterisks indicate correlations significantly different from zero ($p < 0.05$)."
)
```

```{r correlation_sample_size_pairs, echo = FALSE, results = "hide", warning = FALSE, fig.cap = correlation_scatter_cap, fig.dim = c(12, 12)}
readd(corr_ss_plot_gg)
```

```{r correlation_range_analysis, echo = FALSE, results = "hide", warning = FALSE, fig.cap = correlation_range_cap, fig.dim = c(12, 12)}
readd(corr_range_plot_gg)
```

\clearpage

# Method S1: Estimating trait means and covariances through multiple imputation {-}

## Introduction {-}

Under _single_ imputation, the imputation step occurs once outside of the MCMC loop.
In _multiple_ imputation, as implemented in this paper, the imputation step is part of the MCMC loop,
such that imputed data values are conditioned on the current state of the parameters and vice-versa.
In the figure below,
$Y$ is the original data (with missing values),
$Y^*$ is the original data with missing values imputed,
$\mu_p$ and $\Sigma_p$ are values of the mean vector and covariance matrix (respectively) calculated only from the original data,
and $\mu^*_t$ and $\Sigma^*_t$ are the draws of the mean vector and covariance matrix (respectively) at MCMC iteration $t$.

```{tikz diagram, echo = FALSE}
\usetikzlibrary{positioning, calc, fit}
\begin{tikzpicture}[
	every node/.style = {shape = rectangle, draw = black},
	scale = 1.1
]
\node (Y) at (-1, 3) {Data $Y$};
\node (data params) at (-1, 2) {Calculate $\mu_p, \Sigma_p$};
\node (Ystar) at (-1, 1) {Impute $Y^* \mid \mu_p, \Sigma_p$};
\node (guess params) [align = center, right = of Ystar] {Initial guess\\$\mu^*_0$ $\Sigma^*_0$};
\node (mu star) at (0, 0) {Draw $\mu^*_t \mid Y^*, \Sigma^*_{t-1}$};
\node (sigma star) at (0, -1) {Draw $\Sigma^*_t \mid Y^*, \mu^*_t$};
\path [->] (Y) edge (data params);
\path [->] (data params) edge (Ystar);
\path [->] (Ystar) edge (mu star);
\path [->] (guess params) edge (mu star);
\path [->] (mu star) edge (sigma star);
\path [->, out = 0, in = 0] (sigma star.east) edge
	node (t plus one) [right, draw = none, font = {\footnotesize}] {$t = t + 1$}
	(mu star.east);
\node (mcmc loop) [fit = (mu star)(sigma star)(t plus one), shape = rectangle, draw = black!50] {};
\node (mcmc loop text) [align = center, anchor = west, draw = none] at (mcmc loop.east) {MCMC\\loop};
\node (title) [anchor = north, draw = none] at (mcmc loop.south) {Single imputation};
\end{tikzpicture}
\hspace{1cm}
\begin{tikzpicture}[
	every node/.style = {shape = rectangle, draw = black},
	scale = 1.1
]
\node (Y) at (-1, 1) {Data $Y$};
\node (guess params) [align = center] at (1, 1) {Initial guess\\$\mu^*_0$ $\Sigma^*_0$};
\node (Ystar) at (0, 0) {Impute $Y^*_t \mid Y, \mu^*_{t-1}, \Sigma^*_{t-1}$};
\node (mu star) at (0, -1) {Draw $\mu^*_t \mid Y^*_t, \Sigma^*_{t-1}$};
\node (sigma star) at (0, -2) {Draw $\Sigma^*_t \mid Y^*_t, \mu^*_t$};
\path [->] (Y) edge (Ystar);
\path [->] (guess params) edge (Ystar);
\path [->] (Ystar) edge (mu star);
\path [->] (mu star) edge (sigma star);
\path [->, out = 0, in = 0] (sigma star.east) edge
	node (t plus one) [right, draw = none, font = {\footnotesize}] {$t = t + 1$}
	(Ystar.east);
\node (mcmc loop) [fit = (Ystar)(mu star)(sigma star)(t plus one), shape = rectangle, draw = black!50] {};
\node (mcmc loop text) [align = center, anchor = west, draw = none] at (mcmc loop.east) {MCMC\\loop};
\node (title) [anchor = north, draw = none] at (mcmc loop.south) {Multiple imputation};
\end{tikzpicture}
```

## Demonstration {-}

Consider two positively correlated traits, A and B.

```{r create_traits}
set.seed(12345) # For reproducibility
library(mvtraits)
true_mu <- c(0, 0)
true_cov <- matrix(c(1, 0.8, 0.8, 1), nrow = 2)
Y_all <- random_mvnorm(1000, true_mu, true_cov)
colnames(Y_all) <- c("A", "B")
plot(Y_all, pch = 19)
```

Simulate missingness by randomly removing half of the data.

```{r remove_na}
Y <- Y_all
ymiss <- sample.int(2000, 1000)
# Save true values for validation
ymiss_true <- Y[ymiss]
Y[ymiss] <- NA
plot(Y_all, pch = 19, col = "grey")
points(Y, pch = 19, col = "black")
legend(
  "topleft",
  legend = c("True missing", "Present"),
  col = c("grey", "black"),
  pch = 19,
  bg = "white"
)
```

Set an uninformative multivariate normal prior on $\mu$...
```{r prior_mu}
mu_0 <- c(0, 0)
Sigma_0 <- diag(10, 2)
```

...and an uninformative Wishart prior on $\Sigma$.
```{r prior_sigma}
v0 <- 2
S0 <- diag(10, 2)
```

Take an initial guess at the mean and covariance by drawing from their priors.

```{r initial}
mu_star <- random_mvnorm(1, mu_0, Sigma_0)[1,]
mu_star
Sigma_star <- rWishart(1, v0, S0)[,,1]
Sigma_star
```

Impute values based on this initial guess.

```{r impute}
Ystar <- mvnorm_fill_missing(Y, mu_star, Sigma_star)

mylegend <- function() {
  legend(
    "topleft",
    legend = c("True missing", "Present", "Imputed"),
    col = c("grey", "black", "red"),
    pch = 19,
	bg = "white"
  )
}
plot(Y_all, pch = 19, col = "grey")
points(Ystar, pch = 19, col = "red")
points(Y, pch = 19, col = "black")
mylegend()
```

Clearly, these initial values are a bad fit to the data.
Nevertheless, draw $\mu$ and $\Sigma$ conditioned on this set of imputed values.

```{r draw}
mu_star <- draw_mu(Ystar, Sigma_star, mu_0, Sigma_0)
mu_star
Sigma_star <- draw_Sigma(Ystar, mu_star, v0, S0)
Sigma_star
```

Draw a new set of imputed values, conditioned on the current $\mu^*$ and $\Sigma^*$.

```{r impute_2}
Ystar <- mvnorm_fill_missing(Y, mu_star, Sigma_star)
plot(Y_all, pch = 19, col = "grey")
points(Ystar, pch = 19, col = "red")
points(Y, pch = 19, col = "black")
mylegend()
```

This is still a bad fit to the data, but, though it is hard to tell, it has improved slightly.
Doing this repeatedly (in a loop) gradually improves the estimates of the covariance.

```{r loop}
par(mfrow = c(5, 4), mar = c(3, 3, 2, 0.1), cex = 0.4)
for (i in 1:20) {
  mu_star <- draw_mu(Ystar, Sigma_star, mu_0, Sigma_0)[1,]
  Sigma_star <- draw_Sigma(Ystar, mu_star, v0, S0)
  Ystar <- mvnorm_fill_missing(Y, mu_star, Sigma_star)
  title <- sprintf("iteration: %d, covariance: %.2f", i, Sigma_star[2, 1])
  plot(Y_all, pch = 19, col = "grey", xlab = "", ylab = "", main = title)
  points(Ystar, pch = 19, col = "red")
  points(Y, pch = 19, col = "black")
}
```

After a lot of MCMC samples, we can generate a distribution of covariance values, which provides an uncertainty estimate on the covariance.

```{r cov_estimate}
cov_ab <- numeric(1000)
for (i in seq_along(cov_ab)) {
  mu_star <- draw_mu(Ystar, Sigma_star, mu_0, Sigma_0)[1,]
  Sigma_star <- draw_Sigma(Ystar, mu_star, v0, S0)
  Ystar <- mvnorm_fill_missing(Y, mu_star, Sigma_star)
  cov_ab[i] <- Sigma_star[2, 1]
}
hist(cov_ab, xlab = "Covariance estimate", ylab = "Count", main = "Covariance estimate")
abline(v = true_cov[2, 1], col = "red", lwd = 2)
legend("topright", legend = "True cov.", lty = "solid", lwd = 2, col = "red")
```

The `mvtraits` package provides the convenience function `fit_mvnorm` for performing this entire procedure automatically.


```{r}
results <- fit_mvnorm(Y)
results$stats$mu$Mean
true_mu
results$stats$Sigma$Mean
true_cov
```

It also provides a function for bootstrapping the results of a model fit this way:

```{r}
data_boot <- bootstrap_missing(results, Y)
boot_means <- apply(data_boot, c(1, 2), mean)[ymiss]
plot(ymiss_true ~ boot_means, pch = 19,
     xlab = "Imputed values (bootstrapped mean)",
     ylab = "True missing values")
abline(0, 1, lty = "dashed")
```

We can see that for all data points where at least one observation was available as constraint, the algorithm imputed values that were close to the true values.
However, where values for both traits were missing, the algorithm falls back to the global mean, which has the same statistical properties as the missing data (i.e. central tendency and spread) but doesn't match the true values.
