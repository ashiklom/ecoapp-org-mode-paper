---
title: Supporting information
author:
  - >
    Alexey N. Shiklomanov,
    Elizabeth M. Cowdery,
    Michael Bahn,
    Chaeho Byun,
    Steven Jansen,
    Koen Kramer,
    Vanessa Minden,
    Ãœlo Niinemets,
    Yusuke Onoda,
    Nadejda A. Soudzilovskaia,
    Michael C. Dietze
output: pdf_document
header-includes:
  - \usepackage{booktabs}
  - \usepackage{longtable}
  - \usepackage{array}
  - \usepackage{multirow}
  - \usepackage{wrapfig}
  - \usepackage{float}
  - \usepackage{colortbl}
  - \usepackage{pdflscape}
  - \usepackage{tabu}
  - \usepackage{threeparttable}
  - \usepackage{threeparttablex}
  - \usepackage[normalem]{ulem}
  - \usepackage{makecell}
  - \renewcommand{\thetable}{S\arabic{table}}
  - \renewcommand{\thefigure}{S\arabic{figure}}
---

```{r libraries, include = FALSE}
library(knitr)
library(kableExtra)
# library(drake)
library(dplyr, exclude = "group_rows")
library(tidyverse)
## library(GGally, exclude = "nasa")
stopifnot(
  requireNamespace("shiklomanov2017np", quietly = TRUE)
)
npdir <- here::here("..", "np-trait-analysis")
```

```{r mean-compare-all, echo = FALSE, fig.cap = mca_cap}
mca_cap <- paste(
  "Mean and 95% credible intervals on estimates of trait means for each plant functional type",
  "from the univariate, multivariate, and hierarchical models.",
  "For leaf lifespan and SLA, results were similar for mass- and area-based models,",
  "so only results from the mass-based model area shown.",
  "For some PFT-trait combination, where error bars are large due to the relatively",
  "uniformative prior, $y$ axes are constrained to facilitate comparison."
)
knitr::include_graphics(file.path(npdir, "figures", "manuscript", "mean_comparison_all.pdf"))
```

```{r echo = FALSE, results = "asis"}
prior_df <- tibble::tribble(
  ~param, ~mean, ~stdev,
  "Jmax_area", 2, 0.19,
  "Jmax_mass", 0, 0.26,
  "leaf_lifespan", 0.8, 0.4,
  "Narea", 0.25, 0.3,
  "Nmass", 1.25, 0.3,
  "Parea", -1, 0.3,
  "Pmass", 0, 0.3,
  "Rdarea", log10(0.0011), 0.15,
  "Rdmass", log10(0.8), 0.20,
  "SLA", 1, 0.6,
  "Vcmax_mass", log10(0.45), 0.2,
  "Vcmax_area", log10(50), 0.18
) %>%
  dplyr::mutate(variance = stdev ^ 2)

prior_df %>%
  dplyr::mutate(param = factor(param, shiklomanov2017np::both_params),
                mean = sprintf("%.3f", mean)) %>%
  dplyr::arrange(param) %>%
  dplyr::select(Trait = param, "Lognormal Mean" = mean, "Lognormal SD" = stdev) %>%
  knitr::kable(
    format = "latex",
    booktabs = TRUE,
    caption = paste(
      "Parameters for trait-specific prior distributions.",
      "For the multivariate and hierarchical distributions,",
      "the variance-covariance matrix prior was a diagonal matrix containing the",
      "square of the lognormal standard deviation values (i.e. the variance)",
      "shown here and all off-diagnoal terms set to zero." 
    )
  ) %>%
  kableExtra::kable_styling(latex_options = c("hold_position"))
```


```{r mean_value_table, echo = FALSE, results = "asis"}
table_cap_mass <- paste(
  "Mean and 95\\% confidence interval of trait estimates",
  "for mass-normalized traits from the hierarchical model.",
  "Note that some traits for some PFTs have effectively no constraint relative to the mostly uninformative prior (as evidenced by their very large confidence intervals),",
  "so their values should be used with caution."
)

table_cap_area <- paste(
  "Mean and 95\\% confidence interval of trait estimates",
  "for area-normalized traits from the hierarchical model.",
  "Note that some traits for some PFTs have effectively no constraint relative to the mostly uninformative prior (as evidenced by their very large confidence intervals),",
  "so their values should be used with caution.",
  "See mass-normalized table for SLA and leaf lifespan estimates."
)

with_clm <- readRDS(file.path(npdir, "extdata", "tidy-means.rds"))

# Mass-normalized table
with_clm %>%
  filter(mass_area == "mass",
         model_type == "hierarchical") %>%
  select(pft, param, mu_hi, mu_lo, mu_mean) %>%
  # Fix units
  mutate_at(
    vars(starts_with("mu")),
    ~if_else(param %in% c("Nmass", "Pmass"), . * 1000, .)
  ) %>%
  mutate(string = sprintf("%.2f (%.2f, %.2f)",
                          mu_mean, mu_lo, mu_hi)) %>%
  select(pft, param, string) %>%
  spread(param, string) %>%
  arrange(pft) %>%
  kable(
    caption = table_cap_mass,
    format = "latex",
    col.names = linebreak(c(
      "PFT",
      "Leaf lifespan\n(months)",
      "SLA\n(m$^2$ kg$^{-1}$)",
      "$N_\\textrm{mass}$\n(mg g$^{-1}$)",
      "$P_\\textrm{mass}$\n(mg g$^{-1}$)",
      "$R_\\textrm{d, mass}$\n($\\mu$mol g$^{-1}$ s$^{-1}$)",
      "$V_\\textrm{c, max, mass}$\n($\\mu$mol g$^{-1}$ s$^{-1}$)",
      "$J_\\textrm{max, mass}$\n($\\mu$mol g$^{-1}$ s$^{-1}$)"
    )),
    escape = FALSE,
    booktabs = TRUE
  ) %>%
  kableExtra::kable_styling(latex_options = c("scale_down", "hold_position"))

with_clm %>%
  filter(mass_area == "area",
         model_type == "hierarchical") %>%
  select(pft, param, mu_hi, mu_lo, mu_mean) %>%
  mutate(string = sprintf("%.2f (%.2f, %.2f)",
                          mu_mean, mu_lo, mu_hi)) %>%
  select(pft, param, string) %>%
  spread(param, string) %>%
  arrange(pft) %>%
  kable(
	caption = table_cap_area,
	format = "latex",
    col.names = linebreak(c(
      "PFT",
      ## "Leaf lifespan\n(months)",
      ## "SLA\n(m$^2$ kg$^{-1}$)",
      "$N_\\textrm{area}$\n(g m$^{-2}$)",
      "$P_\\textrm{area}$\n(g m$^{-2}$)",
      "$R_\\textrm{d, area}$\n($\\mu$mol m$^{-2}$ s$^{-1}$)",
      "$V_\\textrm{c, max, area}$\n($\\mu$mol m$^{-2}$ s$^{-1}$)",
      "$J_\\textrm{max, area}$\n($\\mu$mol m$^{-2}$ s$^{-1}$)"
    )),
	escape = FALSE,
	booktabs = TRUE
) %>%
  kableExtra::kable_styling(latex_options = c("scale_down", "hold_position"))
```

```{r pairwise_correlation_table, echo = FALSE, results = "asis"}
rma_display <- readRDS(file.path(npdir, "extdata", "rma-display.rds"))
kable(
  rma_display,
  "latex",
  booktabs = TRUE, longtable = TRUE,
  caption = paste("Pairwise reduced major axis (RMA) slope values",
                  "(Mean and 95pct credible interval)",
                  "for each plant functional type.",
                  "Values with asterisks indicate slopes",
                  "with credible intervals that don't intersect zero.")
)

## corr_results <- readd(corr_processed)
## kable(
##   corr_results %>%
##     dplyr::select(pft, yvar, xvar, corr_value, present, missing),
##   "latex",
##   booktabs = TRUE, longtable = TRUE, escape = TRUE,
##   col.names = c("PFT", "Trait 1", "Trait 2", "Correlation (95% CI)", "present", "missing"),
##   caption = "Pairwise trait correlation values for each plant functional type. Values with asterisks indicate correlations significantly different from zero ($p < 0.05$)."
## )
```

\clearpage

# Method S1: Estimating trait means and covariances through multiple imputation {-}

## Introduction {-}

Under _single_ imputation, the imputation step occurs once outside of the MCMC loop.
In _multiple_ imputation, as implemented in this paper, the imputation step is part of the MCMC loop,
such that imputed data values are conditioned on the current state of the parameters and vice-versa.
In the figure below,
$Y$ is the original data (with missing values),
$Y^*$ is the original data with missing values imputed,
$\mu_p$ and $\Sigma_p$ are values of the mean vector and covariance matrix (respectively) calculated only from the original data,
and $\mu^*_t$ and $\Sigma^*_t$ are the draws of the mean vector and covariance matrix (respectively) at MCMC iteration $t$.

```{tikz diagram, echo = FALSE}
\usetikzlibrary{positioning, calc, fit}
\begin{tikzpicture}[
	every node/.style = {shape = rectangle, draw = black},
	scale = 1.1
]
\node (Y) at (-1, 3) {Data $Y$};
\node (data params) at (-1, 2) {Calculate $\mu_p, \Sigma_p$};
\node (Ystar) at (-1, 1) {Impute $Y^* \mid \mu_p, \Sigma_p$};
\node (guess params) [align = center, right = of Ystar] {Initial guess\\$\mu^*_0$ $\Sigma^*_0$};
\node (mu star) at (0, 0) {Draw $\mu^*_t \mid Y^*, \Sigma^*_{t-1}$};
\node (sigma star) at (0, -1) {Draw $\Sigma^*_t \mid Y^*, \mu^*_t$};
\path [->] (Y) edge (data params);
\path [->] (data params) edge (Ystar);
\path [->] (Ystar) edge (mu star);
\path [->] (guess params) edge (mu star);
\path [->] (mu star) edge (sigma star);
\path [->, out = 0, in = 0] (sigma star.east) edge
	node (t plus one) [right, draw = none, font = {\footnotesize}] {$t = t + 1$}
	(mu star.east);
\node (mcmc loop) [fit = (mu star)(sigma star)(t plus one), shape = rectangle, draw = black!50] {};
\node (mcmc loop text) [align = center, anchor = west, draw = none] at (mcmc loop.east) {MCMC\\loop};
\node (title) [anchor = north, draw = none] at (mcmc loop.south) {Single imputation};
\end{tikzpicture}
\hspace{1cm}
\begin{tikzpicture}[
	every node/.style = {shape = rectangle, draw = black},
	scale = 1.1
]
\node (Y) at (-1, 1) {Data $Y$};
\node (guess params) [align = center] at (1, 1) {Initial guess\\$\mu^*_0$ $\Sigma^*_0$};
\node (Ystar) at (0, 0) {Impute $Y^*_t \mid Y, \mu^*_{t-1}, \Sigma^*_{t-1}$};
\node (mu star) at (0, -1) {Draw $\mu^*_t \mid Y^*_t, \Sigma^*_{t-1}$};
\node (sigma star) at (0, -2) {Draw $\Sigma^*_t \mid Y^*_t, \mu^*_t$};
\path [->] (Y) edge (Ystar);
\path [->] (guess params) edge (Ystar);
\path [->] (Ystar) edge (mu star);
\path [->] (mu star) edge (sigma star);
\path [->, out = 0, in = 0] (sigma star.east) edge
	node (t plus one) [right, draw = none, font = {\footnotesize}] {$t = t + 1$}
	(Ystar.east);
\node (mcmc loop) [fit = (Ystar)(mu star)(sigma star)(t plus one), shape = rectangle, draw = black!50] {};
\node (mcmc loop text) [align = center, anchor = west, draw = none] at (mcmc loop.east) {MCMC\\loop};
\node (title) [anchor = north, draw = none] at (mcmc loop.south) {Multiple imputation};
\end{tikzpicture}
```

## Demonstration: Simulated bivariate data {-}

Consider two positively correlated traits, A and B.

```{r create_traits}
set.seed(12345) # For reproducibility
library(mvtraits)
true_mu <- c(0, 0)
true_cov <- matrix(c(1, 0.8, 0.8, 1), nrow = 2)
Y_all <- random_mvnorm(1000, true_mu, true_cov)
colnames(Y_all) <- c("A", "B")
plot(Y_all, pch = 19)
```

Simulate missingness by randomly removing half of the data.

```{r remove_na}
Ym <- Y_all

# Remove half of the data
Ym[sample.int(2000, 1000)] <- NA

# Remove completely missing rows -- they are useless
ikeep <- apply(Ym, 1, function(x) any(!is.na(x)))
Y <- Ym[ikeep, ]

# Save true values for validation
ymiss <- which(is.na(Y))
ymiss_true <- Y_all[ikeep,][ymiss]

plot(Y_all, pch = 19, col = "grey")
points(Y, pch = 19, col = "black")
legend(
  "topleft",
  legend = c("True missing", "Present"),
  col = c("grey", "black"),
  pch = 19,
  bg = "white"
)
```

Set an uninformative multivariate normal prior on $\mu$...
```{r prior_mu}
mu_0 <- c(0, 0)
Sigma_0 <- diag(10, 2)
```

...and an uninformative Wishart prior on $\Sigma$.
```{r prior_sigma}
v0 <- 2
S0 <- diag(10, 2)
```

Take an initial guess at the mean and covariance by drawing from their priors.

```{r initial}
mu_star <- random_mvnorm(1, mu_0, Sigma_0)[1,]
mu_star
Sigma_star <- rWishart(1, v0, S0)[,,1]
Sigma_star
```

Impute values based on this initial guess.

```{r impute}
Ystar <- mvnorm_fill_missing(Y, mu_star, Sigma_star)

mylegend <- function() {
  legend(
    "topleft",
    legend = c("True missing", "Present", "Imputed"),
    col = c("grey", "black", "red"),
    pch = 19,
	bg = "white"
  )
}
plot(Y_all, pch = 19, col = "grey")
points(Ystar, pch = 19, col = "red")
points(Y, pch = 19, col = "black")
mylegend()
```

Clearly, these initial values are a bad fit to the data.
Nevertheless, draw $\mu$ and $\Sigma$ conditioned on this set of imputed values.

```{r draw}
mu_star <- draw_mu(Ystar, Sigma_star, mu_0, Sigma_0)
mu_star
Sigma_star <- draw_Sigma(Ystar, mu_star, v0, S0)
Sigma_star
```

Draw a new set of imputed values, conditioned on the current $\mu^*$ and $\Sigma^*$.

```{r impute_2}
Ystar <- mvnorm_fill_missing(Y, mu_star, Sigma_star)
plot(Y_all, pch = 19, col = "grey")
points(Ystar, pch = 19, col = "red")
points(Y, pch = 19, col = "black")
mylegend()
```

This is still a bad fit to the data, but, though it is hard to tell, it has improved slightly.
Doing this repeatedly (in a loop) gradually improves the estimates of the covariance.

```{r loop}
par(mfrow = c(5, 4), mar = c(3, 3, 2, 0.1), cex = 0.4)
for (i in 1:20) {
  mu_star <- draw_mu(Ystar, Sigma_star, mu_0, Sigma_0)[1,]
  Sigma_star <- draw_Sigma(Ystar, mu_star, v0, S0)
  Ystar <- mvnorm_fill_missing(Y, mu_star, Sigma_star)
  title <- sprintf("iteration: %d, covariance: %.2f", i, Sigma_star[2, 1])
  plot(Y_all, pch = 19, col = "grey", xlab = "", ylab = "", main = title)
  points(Ystar, pch = 19, col = "red")
  points(Y, pch = 19, col = "black")
}
```

After a lot of MCMC samples, we can generate a distribution of covariance values, which provides an uncertainty estimate on the covariance.

```{r cov_estimate}
cov_ab <- numeric(1000)
for (i in seq_along(cov_ab)) {
  mu_star <- draw_mu(Ystar, Sigma_star, mu_0, Sigma_0)[1,]
  Sigma_star <- draw_Sigma(Ystar, mu_star, v0, S0)
  Ystar <- mvnorm_fill_missing(Y, mu_star, Sigma_star)
  cov_ab[i] <- Sigma_star[2, 1]
}
hist(cov_ab, xlab = "Covariance estimate", ylab = "Count", main = "Covariance estimate")
abline(v = true_cov[2, 1], col = "red", lwd = 2)
legend("topright", legend = "True cov.", lty = "solid", lwd = 2, col = "red")
```

The `mvtraits` package provides the convenience function `fit_mvnorm` for performing this entire procedure automatically.


```{r}
results <- fit_mvnorm(Y)
results$stats$mu$Mean
true_mu
results$stats$Sigma$Mean
true_cov
```

It also provides a function for bootstrapping the results of a model fit this way:

```{r}
data_boot <- bootstrap_missing(results, Y)
boot_means <- apply(data_boot, c(1, 2), mean)[ymiss]
# 95% CI for imputed values
boot_lo <- apply(data_boot, c(1, 2), quantile, probs = 0.025)[ymiss]
boot_hi <- apply(data_boot, c(1, 2), quantile, probs = 0.975)[ymiss]
plot(
  x = 0, y = 0,
  xlim = range(c(boot_lo, boot_hi)),
  ylim = range(ymiss_true),
  type = "n",
  xlab = "Imputed values (bootstrapped mean)",
  ylab = "True missing values"
)
arrows(boot_lo, ymiss_true, boot_hi, ymiss_true, length = 0.01, code = 3,
       col = "gray70")
points(boot_means, ymiss_true, pch = 19)
abline(0, 1, lty = "dashed")
abline(lm(ymiss_true ~ boot_means), col = "blue")
legend("bottomright", c("1:1", "Regression"),
       lty = c("dashed", "solid"),
       col = c("black", "blue"))
```

We can see that mean estimates of the imputed values are close to the true values, and that the imputed values have non-trivial (but reasonable) uncertainty around them.

## Demonstration: Hierarchical modeling of the Iris dataset {-}

For a real-world application of this, we can use the Iris dataset.
Because this data is grouped by species, we can also demonstrate the hierarchical model
(though with only 3 species, the statistical power for estimating across-species covariance is minimal).

First, an overview of the Iris dataset:

```{r}
summary(iris)
pairs(iris[,1:4], col = iris[,5], pch = 19)
```

Let's also look at the true values of the correlations, which we can compare against the model estimates.

```{r}
```

First, we isolate the groups from the actual data.

```{r}
grp <- iris[, 5]
head(grp)
dat <- as.matrix(iris[, 1:4])
head(dat)
```

As in the previous example, let's introduce 50% random missingness into the data.

```{r}
dat_m <- dat
ndata <- prod(dim(dat_m))
dat_m[sample(ndata, floor(ndata * 0.5))] <- NA

# Again, we only keep rows that have at least 1 value
ikeep <- apply(dat_m, 1, function(x) any(!is.na(x)))
Y <- dat_m[ikeep, ]
grp_m <- grp[ikeep]

# Save true values for validation
ymiss_true <- dat[ikeep, ]
ymiss_true[!is.na(Y)] <- NA
```

Now, fit the hierarchical model.

```{r}
result <- fit_mvnorm_hier(Y, grp_m)
```

We can compare our correlation matrix estimates to the true correlations in the data.

```{r}
result_corr <- lapply(result$stats$Corr_group, "[[", "Mean")
corr_cols <- colnames(result_corr[[1]])
iris_split <- split(iris[, corr_cols], iris[, 5])
true_corr <- lapply(iris_split, cor)

# Take the ratio of our correlation estimate and the true value
true_corr
result_corr
Map("/", result_corr, true_corr)
```

In all cases, we underestimate the true correlations, but the extent varies by trait and species.
The largest underestimates (in relative terms) occur for correlations that were the weakest in the observational data, which makes sense since statistical power for correlation estimates scales super-linearly with sample size.

If our imputation algorithm works correctly, then we expect traits with stronger correlations to be imputed more accurately and precisely than traits with weak correlations.
Let's estimate the missing values to see if this is the case.

```{r}
boot_data <- bootstrap_missing_hier(result, Y, grp_m, progress = FALSE)
boot_means <- apply(boot_data, c(1, 2), mean)
boot_lo <- apply(boot_data, c(1, 2), quantile, probs = 0.025)
boot_hi <- apply(boot_data, c(1, 2), quantile, probs = 0.975)
boot_means[!is.na(Y)] <- NA
boot_lo[!is.na(Y)] <- NA
boot_hi[!is.na(Y)] <- NA

par(mfrow = c(2, 2))
for (i in 1:4) {
  plot(
    0, 0,
    type = "n",
    xlim = range(c(boot_lo[, i], boot_hi[, i]), na.rm = TRUE),
    ylim = range(ymiss_true[, i], na.rm = TRUE),
    xlab = "Estimate",
    ylab = "True value",
    main = colnames(iris)[i]
  )
  arrows(boot_lo[, i], ymiss_true[, i], boot_hi[, i],
         length = 0.01, col = "gray70", code = 3)
  points(boot_means[, i], ymiss_true[, i],
         pch = 19, col = grp_m)
  abline(0, 1, lty = "dashed")
  legend("bottomright", levels(grp_m),
         pch = 19, col = 1:3)
}
```

# Method S2: Biased mean estimates in non-representatively sampled bivariate data

One important result of this manuscript is that accounting for trait covariance can reduce biases in trait sampling.
We can demonstrate this with an idealized case where we know the correct answer.
Consider two traits "A" and "B", with mean zero, unit variance, and strong covariance ($R = 0.8$):

```{r}
library(mvtraits)
mu <- c(0, 0)
sigma <- matrix(c(1, 0.8, 0.8, 1), nrow = 2, ncol = 2)
n <- 5000
sim <- random_mvnorm(n, mu, sigma)
colnames(sim) <- c("A", "B")
plot(sim[, "A"], sim[, "B"], xlab = "A", ylab = "B", pch = 19, cex = 0.3)
```

Now, suppose that trait B is sampled with a strong preference for low values.
We can simulate this by removing a random sample of B weighted by its value along the normal distribution cumulative distribution function.

```{r}
sim2 <- sim
sim2[sample(n, n / 2, prob = pnorm(sim2[, "B"])), "B"] <- NA

colMeans(sim2, na.rm = TRUE)

plot(density(sim[, "B"]), col = "black",
     main = "Kernel density of trait B",
     xlab = "Trait B")
lines(density(sim2[, "B"], na.rm = TRUE), col = "red")
abline(v = mean(sim[, "B"]), col = "black", lty = "dashed")
abline(v = mean(sim2[, "B"], na.rm = TRUE), col = "red", lty = "dashed")
legend("topright", c("True", "Observed"), lty = 1, col = c("black", "red"))
```

We can see that the univariate estimate of trait B, -0.43, is systematically biased.
Now, let's fit these data using our multivariate model.

```{r}
fit <- fit_mvnorm(sim2)
subset(fit$summary_table,
       variable == "mu",
       select = c("variable", "index", "Mean", "2.5%", "97.5%"))
```

We can see that the mean estimate for B is still negatively biased, but much less so than in the univariate case.
